{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189c1345",
   "metadata": {},
   "source": [
    "# System Workflow\n",
    "\n",
    "![Screenshot](./Snapshots/flow_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d2f20",
   "metadata": {},
   "source": [
    "## Step-1 :\n",
    "## Load the Youtube Transcripts based on TimeStamp Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe9d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader # Load the Youtube Transcript\n",
    "from langchain_community.document_loaders.youtube import TranscriptFormat # To Get transcripts as timestamped chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c41f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript not found for this video .\n"
     ]
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi()\n",
    "try:\n",
    "    docs = ytt_api.fetch(\"K4CEsO9r1gU\",languages=['en'])\n",
    "except Exception as e:\n",
    "    print(\"Transcript not found for this video .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696921e",
   "metadata": {},
   "source": [
    "## Step-2\n",
    "## Loading the embedding model and the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8eaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd991188",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer\n",
    "# Initialize a llm model\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# # First load the tokenizer explicitly\n",
    "# tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "# llm1 = HuggingFaceEndpoint(\n",
    "#     repo_id = repo_id,\n",
    "#     temperature = 0.8,\n",
    "#     max_new_tokens=500,\n",
    "# )\n",
    "# llm = ChatHuggingFace(llm=llm1,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model_name = \"Qwen-Qwq-32b\",max_tokens= 4000,model_kwargs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82fda4",
   "metadata": {},
   "source": [
    "## Step-3\n",
    " \n",
    " ## Creating a vectordatabase using the Chroma db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc275395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1368eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma\n",
    "# !pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafda84",
   "metadata": {},
   "source": [
    "## Step-4 Defining the retriever\n",
    "## Using the Metadatabased Filtering for retrievers\n",
    "\n",
    "#### -> this retriever is known as self-query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[index].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aa3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The link of the video\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"start_seconds\",\n",
    "        description=\"The starting second of the video chunk (in seconds as integer)\",\n",
    "        type=\"integer\"  # Changed from string to integer\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"start_timestamp\",\n",
    "        description=\"Human-readable timestamp (HH:MM:SS format)\",\n",
    "        type=\"string\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the base retriever from your vectorstore with increased k\n",
    "base_vectorstore_retriever = vectorstore.as_retriever(\n",
    "    # search_type = \"mmr\",\n",
    "    search_kwargs={\"k\": 12,'lambda_mult':0.5}  # Increase this number as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Transcript of a youtube video\"\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    # base_retriever = base_vectorstore_retriever,\n",
    "    verbose=True,\n",
    "    search_kwargs={\"k\": 12}  # Increase this number as needed\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example only specifies a filter\n",
    "# retriever.invoke(\"Create me a blog post about the video.\")\n",
    "# retriever.invoke(\"what is meant by multi query retriever ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7394ca0",
   "metadata": {},
   "source": [
    "## Step- 5 Creating tools\n",
    "\n",
    "### Tool A. VectorStore Retriever tool (Convert the rag_chain into a tool)\n",
    "Redirect to this tool if the user queries is regarding the Video content\n",
    "\n",
    "### Tool B. DuckDuckSeach Tool\n",
    "Redirect to this tool if the user query is general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun #Search user queries Online\n",
    "\n",
    "@tool\n",
    "def retriever_vectorstore_tool(query:str)->str:\n",
    "    \"\"\"Use this tool when the user ask about:\n",
    "    - content of the youtube video\n",
    "    - Any queries specifically about the youtube video \n",
    "    - If the user query involves providing summary , or about specific time stamp ,blog etc\n",
    "    Input should be the exact search query.\n",
    "    The tool will perform a vectorstore search using retriever.\"\"\"\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "@tool\n",
    "def duckducksearch_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool Only when:\n",
    "    - The question is about the current news, affairs etc.\n",
    "    \n",
    "    Input should be the exact search query.\n",
    "    The tool will perform a web search using DuckDuckGo.\n",
    "    \"\"\"\n",
    "    return search.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e1b58",
   "metadata": {},
   "source": [
    "## Step-6 Binding the llm with the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [retriever_vectorstore_tool]\n",
    "llm_with_tools=llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a3371",
   "metadata": {},
   "source": [
    "## Step-7 Define the langgraph workflow with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68436115",
   "metadata": {},
   "source": [
    "### Step-7.1 Define the State (flow of information through nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88051b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict \n",
    "from typing import List \n",
    "from langchain_core.messages import AnyMessage,HumanMessage,SystemMessage #can be either HumanMsg or AImsg or ToolMsg\n",
    "from langgraph.graph.message import add_messages #Append the new messages insted of replacing\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"Represents the state of our graph\"\"\"\n",
    "    messages:Annotated[List[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7947b70",
   "metadata": {},
   "source": [
    "### Step-7.2 Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    " # ToolNode is pre-built component that will invoke/execute the tool in behalf of the user and returns the tool_response\n",
    " # tools_condition is pre-built component that routes to ToolNode if the last message has tool call , otherwise routes to end\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display #to visualize the Graph\n",
    "from langchain_core.messages import trim_messages # Trim the message and keep past 2 conversation\n",
    "from langgraph.checkpoint.memory import MemorySaver #Implement langgraph memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1b6a4",
   "metadata": {},
   "source": [
    "#### Step- 7.2.1 \n",
    "#### Implement a ConversationalWindowBuffer Memory using langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that determines the best tool to server the user query \n",
    "def tool_calling_llm(State:State)->State:\n",
    "    selected_msg = trim_messages(\n",
    "        State[\"messages\"],\n",
    "        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
    "        max_tokens=10,  # <-- allow up to 10 messages (includes all AI ,human, tool msg : So have context about 2 previous conversations)\n",
    "        strategy=\"last\",\n",
    "        start_on=\"human\",\n",
    "        # Usually, we want to keep the SystemMessage\n",
    "        # if it's present in the original history.\n",
    "        # The SystemMessage has special instructions for the model.\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "    )\n",
    "    return {'messages':llm_with_tools.invoke(selected_msg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(tools=tools)\n",
    "tool_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423af05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the StateGraph\n",
    "builder = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "#Adding the nodes\n",
    "builder.add_node('tool_calling_llm',tool_calling_llm) # returns the tools that is to be used\n",
    "builder.add_node('tools',tool_node) # Executes the specified tool\n",
    "\n",
    "#Adding Edges\n",
    "builder.add_edge(START,'tool_calling_llm')\n",
    "builder.add_conditional_edges(\n",
    "    'tool_calling_llm',\n",
    "    # If the latest message from AI is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message from AI is a not a tool call -> tools_condition routes to LLM, then generate final response and END\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge('tools','tool_calling_llm')\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "#Compile the graph\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f46fd",
   "metadata": {},
   "source": [
    "### Code Explaination / Flow :\n",
    "\n",
    "    1. Starts with calling the 'tool_calling_llm' , which decides which tool is to be used to answer the user query .\n",
    "\n",
    "    2. It is redirected to the 'tools_condition' function , where \n",
    "        \n",
    "        Case I: If the Last 'AI Message' is a tool call ,then 'tools_conditions' automatically routes to 'tool_node' which will executes the specified tool and return the tool_response.\n",
    "\n",
    "        Case II: If the last 'AI Message' is not a tool call, then 'tools_conditions' routes to 'tool_calling_llm' generates the final reponse and route to END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b12405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Unique Id for each user conversation\n",
    "import uuid\n",
    "thread_id = uuid.uuid4()\n",
    "print(thread_id)\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain in detail what is explained between minute 8 to 20 ?\"\n",
    "response = graph.invoke({\n",
    "    'messages': query\n",
    "},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a27982",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8687288",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in response['messages']:\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_conversation_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
