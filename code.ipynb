{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189c1345",
   "metadata": {},
   "source": [
    "# System Workflow\n",
    "\n",
    "![Screenshot](./Snapshots/flow_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d2f20",
   "metadata": {},
   "source": [
    "## Step-1 :\n",
    "## Load the Youtube Transcripts based on TimeStamp Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe9d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader # Load the Youtube Transcript\n",
    "from langchain_community.document_loaders.youtube import TranscriptFormat # To Get transcripts as timestamped chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21c41f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 56 transcript chunks\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    loader = YoutubeLoader.from_youtube_url(\n",
    "        \"https://www.youtube.com/watch?v=5KLyjoH8uew&t=2954s\",\n",
    "         language=[\"hi\"],\n",
    "         translation=\"en\",\n",
    "        transcript_format=TranscriptFormat.CHUNKS,\n",
    "        chunk_size_seconds=60,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    if len(docs) == 1:  # If only one chunk was returned\n",
    "        print(\"docs len is 1 so RecCharTextSplit\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Adjust based on your needs\n",
    "            chunk_overlap=100,\n",
    "            length_function=len\n",
    "        )\n",
    "        docs = text_splitter.split_documents(docs)\n",
    "\n",
    "    if docs:\n",
    "        print(f\"Successfully loaded {len(docs)} transcript chunks\")\n",
    "    else:\n",
    "        print(\"No transcript data was loaded (empty result)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YouTube transcript: {str(e)}\")\n",
    "    docs = None  \n",
    "\n",
    "if docs:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Failed to load transcript, cannot proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2827201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81b6c89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.youtube.com/watch?v=5KLyjoH8uew&t=120s', 'start_seconds': 120, 'start_timestamp': '00:02:00'}, page_content=\"Whether to build a business in investment or Like updating Aadhaar card and PAN card to do the work of the government whether to learn easy methods of youtubeOn September 1, 2001, when over the US The world's biggest attack was happening At the same time, it was Osama who carried out this attack It was done on the eastern side of Afghanistan There is a mountainous area in me called Khost Sitting there with his bodyguards The entire attack was reported on the BBC Arabic Service I was taking updates of each and every detail and there As soon as this attack happened inside the US, then the attack Same day US Intelligence Agencies Which the CIA, FBI and NSA did without any time The investigation of this entire attack wasted The first thing I do is to All the passengers on board the flight what happened to their passports and visas There were records to be taken out and checked I started and by doing this I made small\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73493711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=0s', 'start_seconds': 0, 'start_timestamp': '00:00:00'}\n",
      "Hi guys, my name is Nitesh and you are welcome to my YouTube channel.  In this video also we will continue our lang chain playlist. Uh in the last video, we started studying rag and we focused on discussing the theory around rag.  Did I tell you what rags are?  Why is it needed ?  I also demonstrated comparing racks there using techniques like fine tuning.  And today's video is a continuation of the previous video. Where we will practically create a rag based system using lang chain.  The plan is that I will take a problem statement and create a rig based system around that problem statement and we are going to do all this code in lang chain. So whatever you have read till now in the previous four videos, document loaders, text splitters, vector strings, we will use all these in today's video and using these, we will create a rag base system.  On the whole it's going to be a very interesting video.  Let's\n"
     ]
    }
   ],
   "source": [
    "index= 0\n",
    "print(docs[index].metadata)\n",
    "print(docs[index].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696921e",
   "metadata": {},
   "source": [
    "## Step-2\n",
    "## Loading the embedding model and the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8eaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd991188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prabal Kuinkel\\Desktop\\Youtube-Conversational_AI\\youtube_ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer\n",
    "# Initialize a llm model\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# # First load the tokenizer explicitly\n",
    "# tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "# llm1 = HuggingFaceEndpoint(\n",
    "#     repo_id = repo_id,\n",
    "#     temperature = 0.8,\n",
    "#     max_new_tokens=500,\n",
    "# )\n",
    "# llm = ChatHuggingFace(llm=llm1,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535bfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e7101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model_name = \"Qwen-Qwq-32b\",max_tokens= 4000,model_kwargs={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82fda4",
   "metadata": {},
   "source": [
    "## Step-3\n",
    " \n",
    " ## Creating a vectordatabase using the Chroma db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc275395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1368eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma\n",
    "# !pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafda84",
   "metadata": {},
   "source": [
    "## Step-4 Defining the retriever\n",
    "## Using the Metadatabased Filtering for retrievers\n",
    "\n",
    "#### -> this retriever is known as self-query retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1fc675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=0s', 'start_seconds': 0, 'start_timestamp': '00:00:00'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[index].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628aa3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The link of the video\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"start_seconds\",\n",
    "        description=\"The starting second of the video chunk (in seconds as integer)\",\n",
    "        type=\"integer\"  # Changed from string to integer\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"start_timestamp\",\n",
    "        description=\"Human-readable timestamp (HH:MM:SS format)\",\n",
    "        type=\"string\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fc7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the base retriever from your vectorstore with increased k\n",
    "base_vectorstore_retriever = vectorstore.as_retriever(\n",
    "    # search_type = \"mmr\",\n",
    "    search_kwargs={\"k\": 12,'lambda_mult':0.5}  # Increase this number as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ae0df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AttributeInfo(name='source', description='The link of the video', type='string'),\n",
       " AttributeInfo(name='start_seconds', description='The starting second of the video chunk (in seconds as integer)', type='integer'),\n",
       " AttributeInfo(name='start_timestamp', description='Human-readable timestamp (HH:MM:SS format)', type='string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_field_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b5d503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Transcript of a youtube video\"\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    # base_retriever = base_vectorstore_retriever,\n",
    "    verbose=True,\n",
    "    search_kwargs={\"k\": 12}  # Increase this number as needed\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d00dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example only specifies a filter\n",
    "# retriever.invoke(\"Create me a blog post about the video.\")\n",
    "# retriever.invoke(\"what is meant by multi query retriever ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7394ca0",
   "metadata": {},
   "source": [
    "## Step- 5 Creating tools\n",
    "\n",
    "### Tool A. VectorStore Retriever tool (Convert the rag_chain into a tool)\n",
    "Redirect to this tool if the user queries is regarding the Video content\n",
    "\n",
    "### Tool B. DuckDuckSeach Tool\n",
    "Redirect to this tool if the user query is general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3bb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun #Search user queries Online\n",
    "\n",
    "@tool\n",
    "def retriever_vectorstore_tool(query:str)->str:\n",
    "    \"\"\"Use this tool when the user ask about:\n",
    "    - content of the youtube video\n",
    "    - Any queries specifically about the youtube video \n",
    "    - If the user query involves providing summary , or about specific time stamp ,blog etc\n",
    "    Input should be the exact search query.\n",
    "    The tool will perform a vectorstore search using retriever.\"\"\"\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "@tool\n",
    "def duckducksearch_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool Only when:\n",
    "    - The question is about the current news, affairs etc.\n",
    "    \n",
    "    Input should be the exact search query.\n",
    "    The tool will perform a web search using DuckDuckGo.\n",
    "    \"\"\"\n",
    "    return search.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e1b58",
   "metadata": {},
   "source": [
    "## Step-6 Binding the llm with the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9144d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools= [retriever_vectorstore_tool]\n",
    "llm_with_tools=llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a3371",
   "metadata": {},
   "source": [
    "## Step-7 Define the langgraph workflow with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68436115",
   "metadata": {},
   "source": [
    "### Step-7.1 Define the State (flow of information through nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88051b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict \n",
    "from typing import List \n",
    "from langchain_core.messages import AnyMessage,HumanMessage,SystemMessage #can be either HumanMsg or AImsg or ToolMsg\n",
    "from langgraph.graph.message import add_messages #Append the new messages insted of replacing\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"Represents the state of our graph\"\"\"\n",
    "    messages:Annotated[List[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7947b70",
   "metadata": {},
   "source": [
    "### Step-7.2 Define the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f1fb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    " # ToolNode is pre-built component that will invoke/execute the tool in behalf of the user and returns the tool_response\n",
    " # tools_condition is pre-built component that routes to ToolNode if the last message has tool call , otherwise routes to end\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from IPython.display import Image, display #to visualize the Graph\n",
    "from langchain_core.messages import trim_messages # Trim the message and keep past 2 conversation\n",
    "from langgraph.checkpoint.memory import MemorySaver #Implement langgraph memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e1b6a4",
   "metadata": {},
   "source": [
    "#### Step- 7.2.1 \n",
    "#### Implement a ConversationalWindowBuffer Memory using langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "456b41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that determines the best tool to server the user query \n",
    "def tool_calling_llm(State:State)->State:\n",
    "    selected_msg = trim_messages(\n",
    "        State[\"messages\"],\n",
    "        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n",
    "        max_tokens=10,  # <-- allow up to 10 messages (includes all AI ,human, tool msg : So have context about 2 previous conversations)\n",
    "        strategy=\"last\",\n",
    "        start_on=\"human\",\n",
    "        # Usually, we want to keep the SystemMessage\n",
    "        # if it's present in the original history.\n",
    "        # The SystemMessage has special instructions for the model.\n",
    "        include_system=True,\n",
    "        allow_partial=False,\n",
    "    )\n",
    "    return {'messages':llm_with_tools.invoke(selected_msg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aa4b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tools(tags=None, recurse=True, explode_args=False, func_accepts_config=True, func_accepts={'store': ('__pregel_store', None)}, tools_by_name={'retriever_vectorstore_tool': StructuredTool(name='retriever_vectorstore_tool', description='Use this tool when the user ask about:\\n    - content of the youtube video\\n    - Any queries specifically about the youtube video \\n    - If the user query involves providing summary , or about specific time stamp ,blog etc\\n    Input should be the exact search query.\\n    The tool will perform a vectorstore search using retriever.', args_schema=<class 'langchain_core.utils.pydantic.retriever_vectorstore_tool'>, func=<function retriever_vectorstore_tool at 0x000001420E0BCC20>)}, tool_to_state_args={'retriever_vectorstore_tool': {}}, tool_to_store_arg={'retriever_vectorstore_tool': None}, handle_tool_errors=True, messages_key='messages')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_node = ToolNode(tools=tools)\n",
    "tool_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "423af05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOydB1hUxxbHZztbWMrSQZQmgt3YMZZg79iJscQYa6LGVBMTY0liy9OoWKJGjQ3sxF4j9rwYK4iCIErvsLC98A7sCyEECMre3bm78/v2u9/szOWy5b9nzpyZe4ZdVlaGCARzw0YEAgYQIRKwgAiRgAVEiAQsIEIkYAERIgELiBCro1bq8tLV8hKdvESr05Zp1DQIb/H4TDaXIbBlC2yZrt58REMYJI5oQF6qTfyjNDlWVpClsnfhCmxZ8L2KHdkaFQ0+H44NszALfjxakOPzeLlvC5FvK6FfKxGiD0SICD6BG8fzs1IUzo1sfFsIvQIEiM6olfrk2NLUJ4r0p4quQyRN29kiOmDtQoz/TXoxMge+sHZvOCDLoqRQAz8wMJN9J7gJxbj7YFYtxCtHclkcFDLEGVkuBdmqYxEZvd909W6GtaW3XiH+ejDH0ZXburs9sgKiN6d3Hihx9bZBuGKlQjz+Y0ajQEGbHlahQgPRm9KbdRAHtsfUZWQi6+PG8TwPP75VqRAYNtPzzqXCvAwVwhKrE2Li3RI4vhZqaUOT+hD+iTe4xWV6HPtAqxNizOHctr2sUYUGfFuKrkXnIfywLiHevVzYrL2YL2IhawUcksS7pTKpFmGGdQkxJU7WZYgjsm66j3C6F1OEMMOKhJjySMbmMFksaxyfVcW7mTD2ejHCDCv6Vp49lPm0FCLT8umnnx4/fhy9PL17987IyEAUwLVhOnvxYAIQ4YQVCbEgR+1nciHGx8ejlycrK6uoiMLes2lbUdpTOcIJaxGiWqnPS1fxRVRNuR47dmzMmDEhISGhoaEff/xxdnY2VLZv3x6s2uLFi3v27AlPdTrd5s2bhw8f3rVr1wEDBixfvlyh+L9ZAvu3b9++OXPmdOnS5erVq4MHD4bKoUOHfvjhh4gChHac3DS8AorWIkQYJ1I38X/37t1ly5aFh4dHRUX98MMPYMw+++wzqD916hQcQZfR0dFQAKnt3Llz1qxZkZGRixYtiomJiYiIMFyBzWYfOXLE399/y5YtHTp0+O6776Byz549S5YsQRQgFLNkUh3CCWtZGCsr1grtqHqzSUlJPB5vyJAhoCcvLy8wdZmZmVBvZ2cHR4FAYCiAFQSDB2qDsre3d9++fa9fv264AoPBsLGxAYtoeCoUlrsQYrHYUDA68FHAB4JwwlqEqNcjLp8q8w9dMChp6tSpw4YN69Spk4eHh0Qi+edp9vb2J0+eBNuZk5Oj1WrlcjlotLK1VatWyFQw2QwYsiCcsJauGTqj4lwNooYmTZrs2LEDbOH69evBsZs8eXJsbOw/T1u1atW2bdvAldy6dSt002FhYVVbRSLTLaiWFWlZbAbCCWsRokDMllM5nRAQEACm7vz58+DksVisefPmqdXqqifASAU8xUmTJg0cONDT09PJyam0tBSZCUo95lfDWoTIF7KcPHlajR5RANi/Bw8eQAEk+Nprr82cORPGK/n5+YZWw0I7vV4PWjQ4i4BMJrty5Urda/CoW6GnkutdGvEQTlhRHBGmmJMfyhAF3LhxY/78+RcvXkxLS3vy5AkMit3d3d3c3HgV3LlzByrBiQwMDDxx4gSck5iYCCYTYj1SqTQlJQX8xWoXhGEKHK9du5acnIwoIOFOiWtjvBbJWpEQfVoIn8VSIsQpU6aAw7d27dpRo0bNnj0bLNm6detAedAE/uKFCxcgZAMhw6+++gqMIviICxYsGDduHJwJYp04cSKMXapdMCgoCGKNa9asWblyJaKAlEdyn+amju3XjRWt0Far9Ce3Z4bN8kTWzYsn8uSHpT1HuSCcsCKLyOUxXbx4dy4VIuvmxi95zbvYIcywrkwPXQdLIj5Kqu3OURhPvPHGGzU2wRCYy+XW2OTj4wOxG0QN9+7dA28SveRLgiE8RIhqbALv0MGV6+yJ10gFWeHNU/evFOn1ZW171qzFkpKSGutVKhV86wa3rxpMJpOi+Q8AxjGV89FGeUknt2e8HuYsduQgzLDGu/hO/ZQZ2N6WXhk5jALOb9waV4kOnOJ+80R+TqoSWRMxh3Ml7lxsf35Wel8zvOvDP6R1HiShe6abegIqdPHmBXUQI1yx0nXz4FqNmtfo93OFcbewWzRvXOAnF70pXezIxlmFiCRhunky71mcHEbTTYLxCvAahdvnC+JuSXuNcfEOxN3wk7R0KD9DdeNEPo/P9Azgw3yDwJb2Ia3cNNXzeNkfFwtbvW7faYAjk4nXQpsaIUL8P+lJiie/lzyLkzm4chxduUI7tlDMFtqxdHgtZK4ZUJq0QCOT6sr0ZQl3Sm2ETP/WIlAhbosO64AIsTpZKYrcdLWsWCuTasGWyEuMqUQICiYnJzdv3hwZFVtHdpm+fM2lrQPbw49v64BdmPBfIUI0KUlJSQsWLDhw4AAi/B2SzJ2ABUSIBCwgQiRgAREiAQuIEAlYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEAhYQIRKwgAiRgAVEiAQsIEIkYAERIgELiBAJWECESMACIkQCFhAhmhQGg+HiglfyakwgQjQpZWVl/9xDgICIEAmYQIRIwAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEAhYQIRKwgAiRgAVEiAQsIEIkYAHZ8McUjBs3TqFQwEet0WgKCgrc3NygrFKpzp49iwgVWOk2uSZm6NChWVlZGRkZubm5Op0uPT0dymIx1vvWmhgiRFMQHh7u5eVVtYbJZIaEhCDCnxAhmgIGgzFy5EgWi1VZ4+3tPXbsWET4EyJEEzFmzJhKowi67NGjh7u7OyL8CRGiiWCz2dBB83g8KIMiR40ahQhVIEI0HSNGjPD09ITxcteuXYk5rAad4oiyYm1+llqroXG8aXjf6WfOnOnVaVxyrAzRFoGIJfHgcrjGtGL0iCOWFGpiDuXmpKq8g0RyqRYRzIpSrpPmqwPa2PYY5YyMBA2EWFqkPbYxvedYdzsnLiJgw6PfCvNeKAdNNY6PQQMhRsx/+taXfkwmAxEwI+GP4rw0Rb+JbqjB4D5Y+e1MfufBzkSFeNL0NTutBmU9V6IGg7sQM5KUto4cRMAVNodRkKlGDQb3UbNOWyZ2IK4hvti78uQlOtRgcBeiTKrVIwK+aNVlZSwjfEVkPSIBC4gQCVhAhEjAAiJEAhYQIRKwgAiRgAVEiAQsIEIkYAERIgELiBAJWECESMACcs9KDQwLC/159zZkbBZ9/cmHH82EQnLy016h7R8+vFe10ugcORoV2qdjtX+NLRZoEb9e/Gnnzt369xuC6MDgwSO0Gg2yeixQiAkJ8SBERBM6tO+MCJYnROjy4Lhi5eKIjd8fj76sVqu3/7Tx18vnCgsLJBKn3qEDJk+azmaXv+s6murJ2bMn9kftysxMd3PzGDd24oD+Q6FSp9P9vHvrxYtncvNyxGK7kK49pk+by+fza7sIdJqlpSXfr94E5bCRfSaMfyc7J+vSr2cVCnnLlm0/mr8QXhs0QT++bv3K5y+eeXh4zZzxwZ692/18A+bN/Qy9JM+fP5s8ZfTKFRv279+ZkBgvFIrenfo+XHP9+pUvUlPc3T0/nL8wqFlzZHIszUc8EHkKju+/9/Ge3dFQWPvD8tNnfpkxfd7OHYfemTL76LGoLT+uM5xZR1N9iLlyceXqJeAArPth++BBYStXLbkccwHqDx3et2//zilTZm3fGvnJx4uu34jZ9lNEPa8JPwNQdpMmvvv3Hv9p24HExMe795S7qiqVauFXHwqEwogNO+fN+Wzbtg2gfgbjVW6fYFX80n7asQlEHH30UquWbdes/Xbnzs1Ll3x/9PAFsa3d+g2rkDmwNIsIRgiOAoHATmxXXFx07vzJGdPnvtGrL1R6eni9ePEMhDLt3fflclltTRxOve5MOHhob7eQnmAIoRzYNKigID8/LxfKYFk7tO/i6+uPyjM6ePfq2fe3/15H9aaxt4/Bsrq4uHbs0PXJk0dQvnnrqlRa/MHcBaBReDrn/U/mzJuKGkCvnn28vZtAoWePPhcunhk4cLiTU/mNod27h27avAaZA0sO3yQlJ0JHGRzUsrImMDBYqVSmpb0oLCqorcnHx68+FwdPFLryyqfTp80xFOzs7EHiq/+zLC8vR6vVQg/L5wtQvfH1Dags29qKpSVSKLx4kSISigwqBFq2bAP/BTUA70ZNDAWwslWfCgVC8FjgZb+Ui2IULFmIYPZQuXUUVtYYNAHiqKOpPlcGyWo0GhubGjw/6NrOXzgF1qt5i9Y8Lm9/5C5w+FC9MSTHqcTQ+4I5NCimEoPhf2XYf7f63L//U7NgyUIETxz9KUcDhjLUq9Sq2prqc2WbCqr+uQGwsqdOR094a2qfPgMNNTJZKWowoE6QftUakCayLCwzoG3IGgDdHIvFio27X1kfF/dAJBJ5ejaqo6me/8LfP/DBgzuVT9dHrIaHXq8HLVaaK5lMduPmlYanMIBXBcpLz0gzPIURNLi/yLKwNCHyKrj/4E7i0yfg8YDjv3ffjmvXLmdnZ0G0JfqXgyNHhIMDBEOZ2prq+Y9GjXzz99u3duzc/PjJo8NHIo8dOxDUrAUMdAL8A8+eOwGiSUpK/HzhvE6dQkpKpODkgeOFXpXOnbrBm9oQsRquAyrctGWtIaZjSVhg1xw+bnJk1K6bN6/u2X0MBpjgCK5dt7yoqNDF2fWt8e+8GT7ZcFodTfWhR/dQiIAcOLgHvEBXV3e4Wu/Q/lD/8UdfrVq9ZMo7YyC4OOXtmaDOuNj7M2dP3LY1Er0qjo6SRV8uj9j0n6nTwn19/N+b/dGq75dyueZ37IwI7rlvdi1N6TPRy9be2hdnFEuLbXg2hqEMDGyHhb0x7d05YcPHIHPz4Eohi6XvPFCCGgZZfUMDSktL35owrF3bjhMnvAtx7KiDu5lMZvfX30AWBBFirSz4Yl5s7L0amwYNDINgODIVMIpasXzD1q3r58x7h8lg+vk3XbUiAtxEmMLZH7mzxj/x9vaJWL8D0QfSNdcKTJWoNTWnFwLn0q5hkTyjUFJaAvPUNTZx2BzDZAnVkK6ZcvAfmdqKbOGBLAIiRAIWECESsIAIkYAFRIgELCBCJGABESIBC4gQCVhAhEjAAiJEAhbgLkSJGxfpabwLpMXD4jBsbFioweC+MJbNZeZlGGFjIwJFZKfI7ZyMsCMT7kL0bSnMz1AhAq4o5TqvpjaoweAuxKbtbLUa3b2YfETAj/O70zv0deRwjdA102O/5gv7stk8lqMbz8nThmwQaXbkJdrCbNWDq4V93nT1CuAjY0APIQJP7pQ8eyjTaMry043cU6vVaiaTaZpbyuHT1qjV1N1HLJfLGQwG809YLCPYqn8ismM7e/Pa9rQXG2+/TtoIkQp0Ot3Tp08vX748ffp0ZBKSkpIWLFhw4MABRA2ff/756dOnQYIODg4ikYjLBl95BQAAEABJREFU5Xp5efn7+8+aNQvhjfUK8eeffx40aJBQKLSxMYKvXU9KSkr++OOPnj17Imp4/Pjx3Llz8/P/cqnLKnB3dz958iTCGCvNGHv48OHCwkKJRGJKFaLydDa21KkQaNasWXBwcNUa6Knhx4a5CpEVCvHSpUtwDAkJAcuBTE5ubu7GjRsRlYSHhzs6OlY+hW766tWrCHusS4jLly9PTk6GgpubGzIHUqkUXFJEJR07dvTz+yuhma+vb3R0NMIeaxEiDErg2K9fv6lTG5RZsIG4uLiYYNwwevRosViMypPmeEZGRt6/f//bb79FeGMVgxUYqIaGhvbu3RtZDePHjwc34Ny5c4an4BMfPXp0z549CFcsXIilpaVFRUWPHj3q27cvwgAQx8GDB80STImPj58wYcKuXbuaNzdDiux/xZK75qVLl+bl5UEgDRMVIpP4iLURFBR0+/btFStWHDp0COGHxQoROqOWLVs2adIE4YRpfMQ6gOhpYmLi4sWLEWZYYNf8448/Tps2DSbuYF4BEWril19+2bt37+7du/H5iCzNIn711Vf29uWJzvFUoQniiPVh6NCh33zzTY8ePe7du4fwwHKEGBMTA8c5c+aMGWP+rIG1YUYfsRowAX3z5s3169fv27cPYYCFCBGiFYYNcJycsM6cZHYfsRrbt2/PzMxcuHAhMje09xHT0tLg24X5EphmRYRX4vTp01u3bgWXUfj3TTRMCY0tolarfffdd5VKJbiDdFEhJj5iNQYMGLBmzRo4/v7778hM0FWIYMivX78+c+ZM8HUQfcDHR6xG48aNr1y5Aj01RLyROaCfEPV6/QcffABChEFfu3btEK3AzUesxubNm4uLiz/55BNkcujnIy5atAgmjrt3744I1HDx4sW1a9eCy2gIhJkGOgkReo1JkyYhOmPGueaXIiMjAyamlyxZEhISgkwCbbrm/v37t2jRAtEcbH3Eanh4eIBdjIqK2rZtGzIJNLCId+7cAV8QRscmXtZPBVTfs2J0Nm3alJCQAGNqRDFYW0SZTNavXz/DGk8LUCGi/p4VowNxibCwMPgWcnJyEJXgaxFLS0sh6O/g4ID5ZMlLQRcfsRp5eXngMi5fvrx169aIGjC1iEeOHIEeOSAgwJJUiCrs+t27dxHdgG8BZl8iIiLS09MRNWCali4xMVGj0SCLA7pmmFlRKBQwM047ZwNMAwxiEDVgahFnzJgxePBgZIlwOBw+nw8DUnA8EH14/PhxYGCgYWUJFWAqRDs7OzNOwJsACIjOmzcP0Yf4+PigoCBEGZgKccuWLSdOnEAWDRhFOKampiI68OjRo2o5JIwLpkKEGU+I3SArICYmBiKLCHuotoiYhm9AiGw227J750qWLVuGw9LUumnfvv3t27cRZRAf0fwYVHjr1i2EK9AvU2oOEfER8SEtLe3s2bMIS6julxHxEfFh1KhRUqkUYQnVIxWErRCnT59uqXHEOhg9ejQc9+/fjzDDei2iVfmI1ZBIJFhlBdHr9TDRBdFsRCXER8SOvn37YpUpxQT9MiI+Ip5ArARVZK1AGGCCfhkRHxFnwsLC9u7di8yNaYSI6eob8BGR1dO2bVtXV1dkbqBrDg8PRxRDfESsMSy7AtOIzIRWq3327FlAQACiGOIj0oDNmzfv3r27ak2/fv2QSTDNSAWRuWa6oK6AxWLx+fyBAwdmZ2eDFk2Qoj0qKur58+cmuOWe+Ij0gFtBt27d4JPJyclhMBhxcXEFBQVVt1ShArCIHTp0QNRDfEQ6AbFusIWGMqjw2rVriGJMM2RGxEekESNHjqx675JcLj9//jyiEnAGUlNTq24fRB2Yds0QRzTNvrV0AVSYkpKCKvbWM9RAAWqSk5N9fX0RNZhspILIXDNdOHz48PDhw729vR0cHAwbjkIldNOU9s4m65cRthYRfERPT08yuVKVL7/8Eo4PHz68WkF+fr60SHH5wm9hQ95E1JDwKLVNmzYlhVr0qsDvRexYL43hFb7p3bt3YWGh4SUZ+iAou7m5nTp1ChGqcPt8wYNrhWUMrUapt+HzETVANBsCRg25hdTRnZeeKPdvLew0UFL3dvd4WcQuXbqcPn266jtnMplDhgxBhCqc2ZUlcuQMmOItsucg7NFq9EU56oM/pI2Y7engUuueI3j5iOPGjas2u+rl5WWCiU4acXpnloMbr3V3CS1UCLA5TCdPmzHzfY5GpEsLas3egZcQmzdvXjUJIpjG/v37mzJvKeakPJJx+azgzg6IhvQa637rVEFtrdiNmidNmlQ5WwDmEOfde0xPTqqKw6Nr/n0HV97TeyW1tWL3riBw1bp1a0OEAswhRCsQ4U9Ucp2TOw/RExab4R0oLMpV19iK48/r7bffhrksGCyPHTsWEaogk+q0dM6RVpCtrm0M3tBRc0aSvDhPKyvRyqU6vQ4G/HpkBCSvN5sFAe3bp1UQtUUNhsdnMhBDIGbBQ+LBc/agq1GxYF5RiM/jZQl3SpNjZQ5u/LIyBovDYsKDxTJWTLJF615wLJEjo1CqQHqtTpeu1amVGmWxRqnzayVs1t7WtbElpEO2DF5aiJnPFFeO5nMEXAab59fFgc1hIbqhVmjz82Qxxwr5AvT6cIm9M9nW2fy8nBAv7M/NSFZKfByFDjS2JVw+27FR+XpHaY7s8PqMoI62XQdLEMGs1HewAvHxnUueK3U873YetFZhVcQuQr8ujXKymBBrRQSzUi8h6rRlPy5Idg92FUkscEWMvaeYYyeOXE2PhJmWyr8LUa8v2/RJUnCoD09IjzmlV0AkEYg9HXcte44IZuLfhbj3uxcBXT2RpSOwt3FsZH9yO50SrFsS/yLEy4fz7BvZ84RWMa60dRFpEO9eTBEimJy6hJifoXoWK7N1FiGrwd7D7tqxPNptHWwB1CXEK8fynXyovVsRQ9yaOlw9lo8IpqVWIWalKLQ6pq2zAGHJ/diLH33ZSSYzfjfq1MQ+PVmlUugQoYJhYaE/76Z8s9xahfj0vgxm7pB1wmCmxBlpetHcfL340zNnjyPsqVWISQ9kti6YmkOqETgKE++VIosgISEe0YGap/gKc9R8Ww51g+W0jMenzm+Eo06rCfDrMHTAB44O7lB/47+Hz178ccpb30ef+k9ObopAYBfa4+1Orw2FJp1OG31qzZ0HZ8r0+uDAbv6+7RFliF0EmXGY5lV/KXqFln9KK1Yujtj4/fHoy1A+eerYgYN7MjLS+HxBp45dZ874wNHx/9ObdTRVAuccOrwvMzOdx7Np3arde7M/cnExTuK8mi1iaZFWqTDKgq4aKCzK2vzTLCaDOXPKxhlTIuRy6Zad72m05eslWUy2Ull6IeanieO+W/rFxdfaDDxyfEVRcfmW1Zeu7Prt9rGhA+Z9MOtnnyZt4BxEGQwGo7RQI5O++m2UmHAgsvzux/ff+3jP7mgonDt3cvX3y/r2GfTTtqglX69KSHy84PO5hhBBHU2VPHhwF84ZOSJ8+7ao7779oVhatHjpZ8hI1CxEuVTHomxZzc3fj8BXPX70UndX/0aeweGjvi4oTH8Yd8nQqtNre70+0d7OFdTQsd0QMIQZWYlQ/8f90y2Ce0CNk6RR144jm/p1QlTCtWHJimkvRLG4fG2HAHqWisLBQ3tDQnqMf/PtRo0at2nzGggUBBcbe7/upkqepSTxeLz+/YZ4engFB7VY9OXy2bM+REaiFiGWaFlcqu40fZEa6+0ZzOfbGp462Ls5OnimZyZUnuDh+v+0kAK+GI5KZYlWq8nLTwXVVp7j7dUcUQmHz5LT3yJWRavVJiUnBge1rKwJDCz/PJ8mJdTRVPUKbdu0B+swZ97UEyePZmZlQMcNckRGola1MRBVQV2FUpaR9eTTr7tV1uh0GmlJXuVTDudvK6ihg1CrFeX17L/qeTxqB1J6XXkPjSwIhVIBn6RA8NeyFQG//DNUKOR1NFW9grd3kw3rduyP2vXj1vUl//kmKKgF+IjG0mLNQhSI2TqNElGDjY3Qx7vNqGF/cy+43LqExeGWLzxTqP4aySoUJYhKdGqdUGxRWaD4NnwmkymX/5VjTVZRFgpFdTRVu4ifX8DCz5fpdLqHD+9t37Hx8y/mHYw6zeEYIcxXc9cssGXpNFRFdBs3apFXkCpx9HJxbmJ4gPER2zrV8SccNtfB3j2zwlk0kJD0X0QlaqVOIKbf4vMaMYw52Gy2v1/Th7H3KusfxT1AFb1wHU1VrxMfHxtXUc9iscCPnPL2zOLiInggY1CzEMWObA6Xqo6pc/swlUoeeWRJesaT3LwX53/dvnpDeGp6XN1/1bZl39hHMbduH8vMehpzfW9GZgKiDL2+TGTPtgCLyKvg/oM7iU+fgCM4evRbt25dgxhNVlbm3Xu310esbt26XbMKtdXRVMlv/73xxZfzY65cTM9IgwseORLp5uoukTghY1DzZ23nxNUqdcoStY2t8UOJEDKcMWXjyXMbIrZNYzJZbi5+b49f3bhRy7r/qs8bU2XyohNn1unL9EFNQwb1fe/nqAVQRhQgzZY5uFjIrFL4uMmRUbtu3ry6Z/ex3qH9VSolqG3rtg3Q7XYL6Tl9+lzDaXU0VfLW+Ckwaty8eW1efi6c06JF6+XfrWMYyZOuNRvYzZP5aSllzr7WeH97RlxOh1BRQFtbhBlndmV5+Il8WtJ1PdTR9c+HzfCwc6rhR17rFJ9/a2GZ1qLiF/WHwdD5NCdpQk1KrW6Qs5cNX1BWnC2zc635K4EJD/Dtamyy4YmUqprnal2dfd6fZsylHAu/Ca2tSa/TMlk1vEGIQU6btK62v8pNLvQJ5rO5dE0xQ1Pq8se7j3A6tDa9NiHaihznz9pdY5NGo6oWC6yEZewVPbW9BkCtUXFrehlsdq2Or16nz31WPHq2KdKXE6pSlxDtJJygTqL83BJb5xq8JRaL7ejggcyNcV+DNLO452jjDAMJL8W/dEBdBzvJ80rlRVQFt7GiOFMqEuqDO5G9hszAv3tCY+d7vbibpVFa+MClKKtUUVDa+00XRDAH9XLJp6/wTbyeasF2sTirFCll4z5qhAhmol5ChKDlrNX+0vQCaTa1M7xmoTC1kMtQDJ9pfn/XmnmJIAUYDIlEl3wrTZpjIZuTFaZLH19+7hPIHjDZDRHMystNp4YMkQR3sr1yND8vSV7G4oidhXTMQ6KQqkpy5XqVysmDM/Drxjy+hSxuoDUvPa/v4MIdNt09K0WZeK806UE2T8DW6xksLqsiVycbYXlrOpPJ0Ki1erVWq9apFRoenxnQRtS0nTPJjIgPr7jAxK2JDTxeH+5UkKUuziu/vUNWrNVpdTotjkLk2jCZLKZQLBCIWU6eXJGdtd4mizENXenk6MaFByIQGgbZipZOCO3YtE564OgGM64195lkap9O8IXMvHQVoicatT4tQWbnVHP/SYRIJ1wb22hUdE3KU5ClqmOJJxEinWjUVMBgoLuXaJms7HiLiukAAACHSURBVNK+jJChtSbNx2u/ZkJ9uHIkV6Mp82sllnjQIKs+RFSKc1W/RmZN+MJbWHu8ggiRlsTeLI67IVXJdUo5VZlhjIKzF68oR+3TUhgyxKnu7SyJEGkMfHVqJdZCLNOX2QjrNXFFhEjAAhJHJGABESIBC4gQCVhAhEjAAiJEAhYQIRKw4H8AAAD///gdsX8AAAAGSURBVAMAV040Bsj1qn0AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initializing the StateGraph\n",
    "builder = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "#Adding the nodes\n",
    "builder.add_node('tool_calling_llm',tool_calling_llm) # returns the tools that is to be used\n",
    "builder.add_node('tools',tool_node) # Executes the specified tool\n",
    "\n",
    "#Adding Edges\n",
    "builder.add_edge(START,'tool_calling_llm')\n",
    "builder.add_conditional_edges(\n",
    "    'tool_calling_llm',\n",
    "    # If the latest message from AI is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message from AI is a not a tool call -> tools_condition routes to LLM, then generate final response and END\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge('tools','tool_calling_llm')\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "#Compile the graph\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f46fd",
   "metadata": {},
   "source": [
    "### Code Explaination / Flow :\n",
    "\n",
    "    1. Starts with calling the 'tool_calling_llm' , which decides which tool is to be used to answer the user query .\n",
    "\n",
    "    2. It is redirected to the 'tools_condition' function , where \n",
    "        \n",
    "        Case I: If the Last 'AI Message' is a tool call ,then 'tools_conditions' automatically routes to 'tool_node' which will executes the specified tool and return the tool_response.\n",
    "\n",
    "        Case II: If the last 'AI Message' is not a tool call, then 'tools_conditions' routes to 'tool_calling_llm' generates the final reponse and route to END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26b12405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a0ec4772-88a1-4530-8559-c3222b3218f5\n"
     ]
    }
   ],
   "source": [
    "# Create a Unique Id for each user conversation\n",
    "import uuid\n",
    "thread_id = uuid.uuid4()\n",
    "print(thread_id)\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What ?\"\n",
    "response = graph.invoke({\n",
    "    'messages': query\n",
    "},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79a27982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is this video about ?', additional_kwargs={}, response_metadata={}, id='130f77f8-58c9-4b39-8331-4ae9d2fb0b99'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fgpd', 'function': {'arguments': '{\"query\": \"What is this video about?\"}', 'name': 'retriever_vectorstore_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 222, 'total_tokens': 423, 'completion_time': 0.467952208, 'prompt_time': 0.016661285, 'queue_time': 0.055607237000000004, 'total_time': 0.484613493}, 'model_name': 'Qwen-Qwq-32b', 'system_fingerprint': 'fp_07cd5d759a', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9a1de5b8-0a1f-4ff2-aeab-9fbd8423d20f-0', tool_calls=[{'name': 'retriever_vectorstore_tool', 'args': {'query': 'What is this video about?'}, 'id': 'call_fgpd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 222, 'output_tokens': 201, 'total_tokens': 423}),\n",
       "  ToolMessage(content='[Document(id=\\'8f2cf85f-058a-4e01-9cda-2298dab0ab58\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=780s\\', \\'start_seconds\\': 780, \\'start_timestamp\\': \\'00:13:00\\'}, page_content=\\'getting a big string like this.  In which the complete transcript of your entire video is visible to you at once.  Ok?  A so this function is not as it is a function. You have to run this chunk of code as it is.  Let me show you a couple of more videos. Like this is our rag video.  If you put this here and run it. So now an error will come here.  See what error is coming?  I will read it for you. Here it shows that the video we have just chosen does not have an English transcript.  And this is a logical thing. Our videos are in Hindi. That is why English transcript is not available.  So what can you do?   You can quickly go here and write Hindi instead of English.  And now as soon as you run this code, look guys the Hindi transcript has been loaded which you will see in our video.  If you turn on captions. Ok?  A I hope you are understanding roughly.  A what I will do is I will be\\'), Document(id=\\'bd3088b9-6837-42f9-ab77-f3efa6e78aa0\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1260s\\', \\'start_seconds\\': 1260, \\'start_timestamp\\': \\'00:21:00\\'}, page_content=\"in our prompt.  So what do we have to do?   The page content inside these four documents , basically this entire string, will have to be concatenated.  So what we are doing is we are writing a custom code where what are we doing? Going to every document.  People are picking up his page content and joining him. Basically we\\'re creating a bigger string. Ok?  So if I show you what that big string or context looks like after this step.  So this is the output.  Ok?  A, this has been trimmed actually.  But this is a very big text.  Ok?  So now I have both things.  I already have my question. Now I also have a much bigger context.  So, I had this thing. Now this has also come.  I also converted it into string format.  Now what we can do is we can invoke prompt. And in invoke we will have to provide two things. First, we will have to give our context.  So the context is this big\"), Document(id=\\'6126297d-d35d-41c2-bc2b-7fbe20c12fd5\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=120s\\', \\'start_seconds\\': 120, \\'start_timestamp\\': \\'00:02:00\\'}, page_content=\\'?  So, I will quickly put a question in our system and my rack based system will quickly reply to me that yes, in this particular video, AI has been discussed and these are key pointers.  Ok?   Can I quickly ask you if you can summarize this entire video in five bullet points?  So, this rack base system of mine will instantly create a summary of the entire video and give it to me.  Tomorrow, if you are watching a lecture on data science and suddenly you have a doubt in some part, then you will quickly enter that doubt in this system and this system will solve that doubt for you. So basically this is a chat system with the help of which you can ask whatever you want about any YouTube video. So I am pretty sure you all will be able to relate to what problem this system is solving. And that is why I thought this could be a very good problem statement for us.  This\\'), Document(id=\\'0a0948c7-ffdd-47f5-98ae-b4b357cbc698\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1320s\\', \\'start_seconds\\': 1320, \\'start_timestamp\\': \\'00:22:00\\'}, page_content=\"string that you see on the screen. And the question which we had above. Ok?  And we will call this final process.  Ok ?  So if I can show you the final plan, look like this.  You are a helpful assistant answers only from the provider in the transcript context.  A: The context starts from here.  Ok?  And finally here comes our question. So, we\\'re going to send this whole thing that you see on the screen right now to our LLM.  Ok?  So we have completed this step also.  This part is also done with argumentation.  Now the last step remains where we will do the generation. So what do you have to do in the last step ?  Simply the LLM you created above.  Ok?  It has to be invoked.  And you have to send your final form there.  After turning around you will get an answer.\"), Document(id=\\'9bba3692-3fde-4527-bf77-249d35f5df62\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=60s\\', \\'start_seconds\\': 60, \\'start_timestamp\\': \\'00:01:00\\'}, page_content=\"start.  So come on guys, first let\\'s talk about the problem statement.  What are we going to make in this video?  So we will work on a very simple yet very important problem statement.  And the name of our problem statement is YouTube Chat.  I don\\'t know may be YouTube chat could be a good name.  So basically what we are going to do is we are going to create a rag base system with the help of which you can chat with any YouTube video in real time. Ok?  So we all watch videos on YouTube.  Right?  Some videos are very long.  Like especially if you are watching any podcast etc. then it is two to three hours in length.   The problem is that if you want to understand the entire content of that video then you will have to watch the entire video.  Our solution solves this problem for you. For example, let’s say I’m watching a podcast which is 3 hours long.   Let me assume, there was a question about that podcast, whether AI is being discussed in this podcast\"), Document(id=\\'4cd4d6cc-a7dc-49b0-9489-c9bf98cb10ec\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=960s\\', \\'start_seconds\\': 960, \\'start_timestamp\\': \\'00:16:00\\'}, page_content=\"convert all these chunks into vectors and store those vectors in a vector store.  So for that I\\'m going to use fi vector store. So what did we do first?  We chose an embedding model which is a model from Open AI Embeddings.  We\\'re using this model a and we\\'re using fires vector store. And we provided all our chunks there.  And our embedding model was provided and we ran it.  So if you see at this point, an ID has been generated against each chunk and with this ID, those chunks have been embedded and stored in our vector store. So in total there are 168 chunks.  If you want to see a particular chunk, you can quickly enter its ID in this function and check how that chunk looks.  So this is the last chunk.  Ok?  So we have completed this step also.  So basically\"), Document(id=\\'c5b4bbf9-1524-46d0-bb5d-27e239de0713\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1380s\\', \\'start_seconds\\': 1380, \\'start_timestamp\\': \\'00:23:00\\'}, page_content=\"If you print that answer directly then you will get something like this.  LLM is not defined.  Maybe I didn\\'t do the above cell run.  Let me run this. And again we run this code. Ok?  So you can see yes the topic of aliens was discussed.  The Speaker expresses their personal opinion on whatever is presented. And after this I am getting a lot of other meta data. If I want just the string I will fetch the content and here is my answer.  Ok?  Here is my ans. So now this part is also working. Generation is also happening.  Ok?  You can try out different questions. Like you can ask is the topic of nuclear fuse discussed in this video?  If yes then what was discussed?  Since I have watched this video , I know these topics have been discussed.  Again we are running the same code.\"), Document(id=\\'d2ba527f-0663-4836-a23b-c0fef77709e9\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=240s\\', \\'start_seconds\\': 240, \\'start_timestamp\\': \\'00:04:00\\'}, page_content=\"work with tools like Streamlet. Ok?  A but since today we\\'re going to focus on rag so I\\'m not going to focus too much on the UI right now.  I would focus more on functionality. And that is why whatever we create will be created inside a Google Collab Notebook. But I would recommend that once you try and run this project in Google Collab, then definitely you should try and build a UI around this project. What will happen with this?  Your project will look a little nicer.  It will become a little more usable. Ok?  So in a nutshell this is the problem statement that we are targeting and we are going to build a rack system that will solve this problem statement. Ok?  Next I will discuss the exact plan of action on how we will solve this problem statement?  Now the interesting thing is that to make this project, we will use exactly the same flow that we discussed in the last video.\"), Document(id=\\'773017a6-f1c3-4d8a-83e5-7b8daebe8927\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1980s\\', \\'start_seconds\\': 1980, \\'start_timestamp\\': \\'00:33:00\\'}, page_content=\\'use a variety of techniques. So I just want to give you a little flavour of how we can improve a simple rack system to the point where it can operate at an industry grade level. Ok?  So what I will do is I will suggest you some improvements from my side.  And I have divided all these improvements into categories.  Ok? So the first category is that we can make some UI based enhancements to our Simple Rack system.  At this point, our rig is running inside of a Google CoLab notebook.  Where the user has to go manually and provide the video ID. All the sales have to be run. Then he gets the answers to his questions. Obviously a finished product, a complete product, will not work this way.  So what can we do ?  We can improve our code in such a way that the final product appears in the form of a website.\\'), Document(id=\\'bfacfd76-dc13-4a48-8893-d94a88c489b1\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=840s\\', \\'start_seconds\\': 840, \\'start_timestamp\\': \\'00:14:00\\'}, page_content=\"using the transcript of this podcast in the process of creating this project.  This is around a 2 hour podcast. So we will ask questions from this podcast later on.  Ok?  By the way, if you have not seen this video, please watch it whatever screen is on it.  It\\'s a very very good video.  A so what I will do is I will paste it here and here I will write the language as English.  Ok ?  So now I have the complete transcript of this 2 hour video. Ok ?  And he is currently standing in this variable. Right?  So we have completed this step one. We can bring the transcript from the YouTube video into our code.  Have been able to bring it.  Ok?  Now we come to step two.  What do we need to do in step two ?  Since this is our transcript, it is the transcript of a 2 hour video. So, obviously it will be very long.  We have to divide it into small chunks.\"), Document(id=\\'7eda909e-51af-44db-b3eb-ab846c012740\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=720s\\', \\'start_seconds\\': 720, \\'start_timestamp\\': \\'00:12:00\\'}, page_content=\"Look at this.  Right now you can see this thing on top of the video.  Now it will come after 7 seconds and will remain on the screen for 5 seconds. Ok?  Then when I play the video further, after some time the next line will be loaded.  This one supposes you also have this magical power machine, whatever it is. Hey look at this.  Ok?  This will come at 13 seconds and will remain on the screen for 3 seconds.  So basically this API is loading your transcript sentence by sentence, that is, which sentence will appear next in the subtitle after one sentence. Now what we have to do is that first we have to join all these sentences which are currently broken. So that\\'s what I did.  I\\'ve run a loop over it and basically called the join function and we\\'ve concatenated that whole string, kind of combined it. Due to which finally you are\"), Document(id=\\'d1570c1c-b340-40d6-b35f-c592b7bb41e6\\', metadata={\\'source\\': \\'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=0s\\', \\'start_seconds\\': 0, \\'start_timestamp\\': \\'00:00:00\\'}, page_content=\"Hi guys, my name is Nitesh and you are welcome to my YouTube channel.  In this video also we will continue our lang chain playlist. Uh in the last video, we started studying rag and we focused on discussing the theory around rag.  Did I tell you what rags are?  Why is it needed ?  I also demonstrated comparing racks there using techniques like fine tuning.  And today\\'s video is a continuation of the previous video. Where we will practically create a rag based system using lang chain.  The plan is that I will take a problem statement and create a rig based system around that problem statement and we are going to do all this code in lang chain. So whatever you have read till now in the previous four videos, document loaders, text splitters, vector strings, we will use all these in today\\'s video and using these, we will create a rag base system.  On the whole it\\'s going to be a very interesting video.  Let\\'s\")]', name='retriever_vectorstore_tool', id='95932fab-73f5-4193-8987-a8fbfd586205', tool_call_id='call_fgpd'),\n",
       "  AIMessage(content='The YouTube video is about building a **RAG (Retrieval-Augmented Generation)-based chat system** using **LangChain** to interact with YouTube videos. Here\\'s a summary of its content:\\n\\n### **Main Focus**:\\n- Creating a **YouTube Chat Application** where users can ask questions about a video\\'s content (e.g., \"Was AI discussed in this video?\") and get answers without watching the entire video.\\n\\n### **Key Steps Demonstrated**:\\n1. **Transcript Extraction**:  \\n   - Fetching the transcript of a YouTube video (e.g., a 2-hour podcast) using APIs and handling language settings (e.g., switching from English to Hindi if needed).\\n   - Merging fragmented transcript segments into a single coherent text.\\n\\n2. **Text Processing**:  \\n   - Splitting the transcript into smaller chunks using text splitters.\\n   - Storing these chunks in a **vector database** (using `FAISS`) with embeddings generated via OpenAI’s model.\\n\\n3. **Query Handling**:  \\n   - Users input a question, which is converted into an embedding.\\n   - The system retrieves the most relevant transcript chunks from the vector database.\\n   - An LLM (Large Language Model) uses the retrieved context to generate an answer.\\n\\n4. **Code Implementation**:  \\n   - Step-by-step coding in Google Colab, including handling errors (e.g., missing transcripts, embedding configurations).\\n   - Final demo showing the system answering questions like, *\"Summarize this video in 5 bullet points\"* or checking if specific topics (e.g., \"aliens\" or \"nuclear fusion\") were discussed.\\n\\n### **Enhancements Discussed**:\\n- **UI Improvements**: Building a web interface (using Streamlit) for a user-friendly experience.\\n- **Performance Optimizations**: Caching transcripts, improving chunking logic, and refining answers with metadata.\\n\\n### **Toolchain**:\\n- **LangChain** components: Document Loaders, Text Splitters, VectorStores, and LLMs.\\n- Libraries like `pytube` for downloading transcripts and `openai` for embeddings.\\n\\nThe video is part of a playlist teaching advanced LangChain applications and aims to make complex videos (e.g., long podcasts) more interactive and searchable.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 996, 'prompt_tokens': 3728, 'total_tokens': 4724, 'completion_time': 2.24764547, 'prompt_time': 0.253860904, 'queue_time': 0.083903053, 'total_time': 2.501506374}, 'model_name': 'Qwen-Qwq-32b', 'system_fingerprint': 'fp_07cd5d759a', 'finish_reason': 'stop', 'logprobs': None}, id='run-d31f3cc6-8bc1-46c3-aa71-7737740b94f9-0', usage_metadata={'input_tokens': 3728, 'output_tokens': 996, 'total_tokens': 4724})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8687288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is this video about ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever_vectorstore_tool (call_fgpd)\n",
      " Call ID: call_fgpd\n",
      "  Args:\n",
      "    query: What is this video about?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever_vectorstore_tool\n",
      "\n",
      "[Document(id='8f2cf85f-058a-4e01-9cda-2298dab0ab58', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=780s', 'start_seconds': 780, 'start_timestamp': '00:13:00'}, page_content='getting a big string like this.  In which the complete transcript of your entire video is visible to you at once.  Ok?  A so this function is not as it is a function. You have to run this chunk of code as it is.  Let me show you a couple of more videos. Like this is our rag video.  If you put this here and run it. So now an error will come here.  See what error is coming?  I will read it for you. Here it shows that the video we have just chosen does not have an English transcript.  And this is a logical thing. Our videos are in Hindi. That is why English transcript is not available.  So what can you do?   You can quickly go here and write Hindi instead of English.  And now as soon as you run this code, look guys the Hindi transcript has been loaded which you will see in our video.  If you turn on captions. Ok?  A I hope you are understanding roughly.  A what I will do is I will be'), Document(id='bd3088b9-6837-42f9-ab77-f3efa6e78aa0', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1260s', 'start_seconds': 1260, 'start_timestamp': '00:21:00'}, page_content=\"in our prompt.  So what do we have to do?   The page content inside these four documents , basically this entire string, will have to be concatenated.  So what we are doing is we are writing a custom code where what are we doing? Going to every document.  People are picking up his page content and joining him. Basically we're creating a bigger string. Ok?  So if I show you what that big string or context looks like after this step.  So this is the output.  Ok?  A, this has been trimmed actually.  But this is a very big text.  Ok?  So now I have both things.  I already have my question. Now I also have a much bigger context.  So, I had this thing. Now this has also come.  I also converted it into string format.  Now what we can do is we can invoke prompt. And in invoke we will have to provide two things. First, we will have to give our context.  So the context is this big\"), Document(id='6126297d-d35d-41c2-bc2b-7fbe20c12fd5', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=120s', 'start_seconds': 120, 'start_timestamp': '00:02:00'}, page_content='?  So, I will quickly put a question in our system and my rack based system will quickly reply to me that yes, in this particular video, AI has been discussed and these are key pointers.  Ok?   Can I quickly ask you if you can summarize this entire video in five bullet points?  So, this rack base system of mine will instantly create a summary of the entire video and give it to me.  Tomorrow, if you are watching a lecture on data science and suddenly you have a doubt in some part, then you will quickly enter that doubt in this system and this system will solve that doubt for you. So basically this is a chat system with the help of which you can ask whatever you want about any YouTube video. So I am pretty sure you all will be able to relate to what problem this system is solving. And that is why I thought this could be a very good problem statement for us.  This'), Document(id='0a0948c7-ffdd-47f5-98ae-b4b357cbc698', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1320s', 'start_seconds': 1320, 'start_timestamp': '00:22:00'}, page_content=\"string that you see on the screen. And the question which we had above. Ok?  And we will call this final process.  Ok ?  So if I can show you the final plan, look like this.  You are a helpful assistant answers only from the provider in the transcript context.  A: The context starts from here.  Ok?  And finally here comes our question. So, we're going to send this whole thing that you see on the screen right now to our LLM.  Ok?  So we have completed this step also.  This part is also done with argumentation.  Now the last step remains where we will do the generation. So what do you have to do in the last step ?  Simply the LLM you created above.  Ok?  It has to be invoked.  And you have to send your final form there.  After turning around you will get an answer.\"), Document(id='9bba3692-3fde-4527-bf77-249d35f5df62', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=60s', 'start_seconds': 60, 'start_timestamp': '00:01:00'}, page_content=\"start.  So come on guys, first let's talk about the problem statement.  What are we going to make in this video?  So we will work on a very simple yet very important problem statement.  And the name of our problem statement is YouTube Chat.  I don't know may be YouTube chat could be a good name.  So basically what we are going to do is we are going to create a rag base system with the help of which you can chat with any YouTube video in real time. Ok?  So we all watch videos on YouTube.  Right?  Some videos are very long.  Like especially if you are watching any podcast etc. then it is two to three hours in length.   The problem is that if you want to understand the entire content of that video then you will have to watch the entire video.  Our solution solves this problem for you. For example, let’s say I’m watching a podcast which is 3 hours long.   Let me assume, there was a question about that podcast, whether AI is being discussed in this podcast\"), Document(id='4cd4d6cc-a7dc-49b0-9489-c9bf98cb10ec', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=960s', 'start_seconds': 960, 'start_timestamp': '00:16:00'}, page_content=\"convert all these chunks into vectors and store those vectors in a vector store.  So for that I'm going to use fi vector store. So what did we do first?  We chose an embedding model which is a model from Open AI Embeddings.  We're using this model a and we're using fires vector store. And we provided all our chunks there.  And our embedding model was provided and we ran it.  So if you see at this point, an ID has been generated against each chunk and with this ID, those chunks have been embedded and stored in our vector store. So in total there are 168 chunks.  If you want to see a particular chunk, you can quickly enter its ID in this function and check how that chunk looks.  So this is the last chunk.  Ok?  So we have completed this step also.  So basically\"), Document(id='c5b4bbf9-1524-46d0-bb5d-27e239de0713', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1380s', 'start_seconds': 1380, 'start_timestamp': '00:23:00'}, page_content=\"If you print that answer directly then you will get something like this.  LLM is not defined.  Maybe I didn't do the above cell run.  Let me run this. And again we run this code. Ok?  So you can see yes the topic of aliens was discussed.  The Speaker expresses their personal opinion on whatever is presented. And after this I am getting a lot of other meta data. If I want just the string I will fetch the content and here is my answer.  Ok?  Here is my ans. So now this part is also working. Generation is also happening.  Ok?  You can try out different questions. Like you can ask is the topic of nuclear fuse discussed in this video?  If yes then what was discussed?  Since I have watched this video , I know these topics have been discussed.  Again we are running the same code.\"), Document(id='d2ba527f-0663-4836-a23b-c0fef77709e9', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=240s', 'start_seconds': 240, 'start_timestamp': '00:04:00'}, page_content=\"work with tools like Streamlet. Ok?  A but since today we're going to focus on rag so I'm not going to focus too much on the UI right now.  I would focus more on functionality. And that is why whatever we create will be created inside a Google Collab Notebook. But I would recommend that once you try and run this project in Google Collab, then definitely you should try and build a UI around this project. What will happen with this?  Your project will look a little nicer.  It will become a little more usable. Ok?  So in a nutshell this is the problem statement that we are targeting and we are going to build a rack system that will solve this problem statement. Ok?  Next I will discuss the exact plan of action on how we will solve this problem statement?  Now the interesting thing is that to make this project, we will use exactly the same flow that we discussed in the last video.\"), Document(id='773017a6-f1c3-4d8a-83e5-7b8daebe8927', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=1980s', 'start_seconds': 1980, 'start_timestamp': '00:33:00'}, page_content='use a variety of techniques. So I just want to give you a little flavour of how we can improve a simple rack system to the point where it can operate at an industry grade level. Ok?  So what I will do is I will suggest you some improvements from my side.  And I have divided all these improvements into categories.  Ok? So the first category is that we can make some UI based enhancements to our Simple Rack system.  At this point, our rig is running inside of a Google CoLab notebook.  Where the user has to go manually and provide the video ID. All the sales have to be run. Then he gets the answers to his questions. Obviously a finished product, a complete product, will not work this way.  So what can we do ?  We can improve our code in such a way that the final product appears in the form of a website.'), Document(id='bfacfd76-dc13-4a48-8893-d94a88c489b1', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=840s', 'start_seconds': 840, 'start_timestamp': '00:14:00'}, page_content=\"using the transcript of this podcast in the process of creating this project.  This is around a 2 hour podcast. So we will ask questions from this podcast later on.  Ok?  By the way, if you have not seen this video, please watch it whatever screen is on it.  It's a very very good video.  A so what I will do is I will paste it here and here I will write the language as English.  Ok ?  So now I have the complete transcript of this 2 hour video. Ok ?  And he is currently standing in this variable. Right?  So we have completed this step one. We can bring the transcript from the YouTube video into our code.  Have been able to bring it.  Ok?  Now we come to step two.  What do we need to do in step two ?  Since this is our transcript, it is the transcript of a 2 hour video. So, obviously it will be very long.  We have to divide it into small chunks.\"), Document(id='7eda909e-51af-44db-b3eb-ab846c012740', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=720s', 'start_seconds': 720, 'start_timestamp': '00:12:00'}, page_content=\"Look at this.  Right now you can see this thing on top of the video.  Now it will come after 7 seconds and will remain on the screen for 5 seconds. Ok?  Then when I play the video further, after some time the next line will be loaded.  This one supposes you also have this magical power machine, whatever it is. Hey look at this.  Ok?  This will come at 13 seconds and will remain on the screen for 3 seconds.  So basically this API is loading your transcript sentence by sentence, that is, which sentence will appear next in the subtitle after one sentence. Now what we have to do is that first we have to join all these sentences which are currently broken. So that's what I did.  I've run a loop over it and basically called the join function and we've concatenated that whole string, kind of combined it. Due to which finally you are\"), Document(id='d1570c1c-b340-40d6-b35f-c592b7bb41e6', metadata={'source': 'https://www.youtube.com/watch?v=J5_-l7WIO_w&t=0s', 'start_seconds': 0, 'start_timestamp': '00:00:00'}, page_content=\"Hi guys, my name is Nitesh and you are welcome to my YouTube channel.  In this video also we will continue our lang chain playlist. Uh in the last video, we started studying rag and we focused on discussing the theory around rag.  Did I tell you what rags are?  Why is it needed ?  I also demonstrated comparing racks there using techniques like fine tuning.  And today's video is a continuation of the previous video. Where we will practically create a rag based system using lang chain.  The plan is that I will take a problem statement and create a rig based system around that problem statement and we are going to do all this code in lang chain. So whatever you have read till now in the previous four videos, document loaders, text splitters, vector strings, we will use all these in today's video and using these, we will create a rag base system.  On the whole it's going to be a very interesting video.  Let's\")]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The YouTube video is about building a **RAG (Retrieval-Augmented Generation)-based chat system** using **LangChain** to interact with YouTube videos. Here's a summary of its content:\n",
      "\n",
      "### **Main Focus**:\n",
      "- Creating a **YouTube Chat Application** where users can ask questions about a video's content (e.g., \"Was AI discussed in this video?\") and get answers without watching the entire video.\n",
      "\n",
      "### **Key Steps Demonstrated**:\n",
      "1. **Transcript Extraction**:  \n",
      "   - Fetching the transcript of a YouTube video (e.g., a 2-hour podcast) using APIs and handling language settings (e.g., switching from English to Hindi if needed).\n",
      "   - Merging fragmented transcript segments into a single coherent text.\n",
      "\n",
      "2. **Text Processing**:  \n",
      "   - Splitting the transcript into smaller chunks using text splitters.\n",
      "   - Storing these chunks in a **vector database** (using `FAISS`) with embeddings generated via OpenAI’s model.\n",
      "\n",
      "3. **Query Handling**:  \n",
      "   - Users input a question, which is converted into an embedding.\n",
      "   - The system retrieves the most relevant transcript chunks from the vector database.\n",
      "   - An LLM (Large Language Model) uses the retrieved context to generate an answer.\n",
      "\n",
      "4. **Code Implementation**:  \n",
      "   - Step-by-step coding in Google Colab, including handling errors (e.g., missing transcripts, embedding configurations).\n",
      "   - Final demo showing the system answering questions like, *\"Summarize this video in 5 bullet points\"* or checking if specific topics (e.g., \"aliens\" or \"nuclear fusion\") were discussed.\n",
      "\n",
      "### **Enhancements Discussed**:\n",
      "- **UI Improvements**: Building a web interface (using Streamlit) for a user-friendly experience.\n",
      "- **Performance Optimizations**: Caching transcripts, improving chunking logic, and refining answers with metadata.\n",
      "\n",
      "### **Toolchain**:\n",
      "- **LangChain** components: Document Loaders, Text Splitters, VectorStores, and LLMs.\n",
      "- Libraries like `pytube` for downloading transcripts and `openai` for embeddings.\n",
      "\n",
      "The video is part of a playlist teaching advanced LangChain applications and aims to make complex videos (e.g., long podcasts) more interactive and searchable.\n"
     ]
    }
   ],
   "source": [
    "for msg in response['messages']:\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
